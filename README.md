# Contradictory-XLM-RoBERTa
The goal is to predict whether a given hypothesis is related to its premise by contradiction, entailment, or whether neither of those is true (neutral).
The [Contradictory, My Dear Watson](https://www.kaggle.com/competitions/contradictory-my-dear-watson) dataset from Kaggle has been used. 

The pretrained model **XLM-RoBERTa** has been used. XLM means Cross-lingual Language Model. XLM-RoBERTa (XLM-R) is a pre-trained multilingual model that outperforms multiligual BERT. One reason for this is that XLM-R was trained using a lot more data. XLM-R was also trained on 100 languages.
